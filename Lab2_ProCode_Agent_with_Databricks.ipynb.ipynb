{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6767558-bd29-417d-8e61-0f370ba35632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 2: Build an HR Q&A Agent with Pro-Code\n",
    "\n",
    "## Our agent is composed of:\n",
    "\n",
    "- [**agent.py**]($./agent.py): in this file, we used Langchain to prepare an agent ready to be used.\n",
    "- [**agent_config.yaml**]($./agent_config.yaml): this file contains our agent configuration, including the system prompt and the LLM endpoint that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ed2740-5eac-446e-937a-fc6f78c415c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow>=3.1.1 langchain langgraph databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa75dd0-589a-4a25-ba09-2c27c47e4fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./resources/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbdae9a-ce9c-4f97-98f7-5074236af90a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Extracting the PDF information\n",
    "Databricks provides a builtin `ai_parse_document` function, leveraging AI to analyze and extract PDF information as text. This makes it super easy to ingest unstructured information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5ff835-0057-4f1f-adca-544255b77875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- The path of the files depends on the volume that you use in the lab 1.\n",
    "SELECT path FROM READ_FILES('/Volumes/wsdb_demos/agent_lab/hr_documents_volume', format => 'binaryFile') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ed82f7-8677-4933-8dca-aea965a118d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT ai_parse_document(content) AS parsed_document\n",
    "  FROM READ_FILES('/Volumes/wsdb_demos/agent_lab/hr_documents_volume', format => 'binaryFile') limit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d98e2b5-534e-4138-9e29-d3ae509abe13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1/ Create our knowledge base table\n",
    "\n",
    "Let's first create our table. We'll enable Change Data Feed so that we can create our vector search on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06745a16-cd38-4fa8-af00-7f7e9dc2283c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Change the catalog and schema to match your environment\n",
    "USE CATALOG wsdb_demos;\n",
    "USE SCHEMA agent_lab;\n",
    "CREATE TABLE IF NOT EXISTS knowledge_base (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  product_name STRING,\n",
    "  title STRING,\n",
    "  content STRING,\n",
    "  doc_uri STRING)\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa164d6-f5f2-45be-8ff4-b49337ea1ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2/ PDF to text with ai_parse_document\n",
    "\n",
    "Let's now use Databricks built in `ai_parse_document` function to automatically parse the PDF document for us, making it super easy to extract the information!\n",
    "\n",
    "*Note: in this case, we have relatively small pdf documents, so we'll merge all the pages of the document in one single text field for our RAG system to work properly. Bigger docs might need some pre-processing steps to potentially reduce context size and be able to search/retreive more documents, adding potential pre-processing steps, for example ensuring the WIFI Router model is present in all the chunk to keep the vector search more relevant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7384573f-369e-4ff3-84cc-ac8dba722291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Change the catalog and schema to match your environment\n",
    "USE CATALOG wsdb_demos;\n",
    "USE SCHEMA agent_lab;\n",
    "INSERT OVERWRITE TABLE knowledge_base (product_name, title, content, doc_uri)\n",
    "SELECT ai_extract.product_name, ai_extract.title, content, doc_uri\n",
    "FROM (\n",
    "  SELECT\n",
    "    ai_extract(content, array('product_name', 'title')) AS ai_extract,\n",
    "    content,\n",
    "    doc_uri\n",
    "  FROM (\n",
    "    SELECT array_join(\n",
    "            transform(parsed_document:document.elements::ARRAY<STRUCT<content:STRING>>, x -> x.content), '\\n') AS content,\n",
    "           path as doc_uri\n",
    "    FROM (\n",
    "      SELECT ai_parse_document(content) AS parsed_document, path\n",
    "      FROM READ_FILES('/Volumes/wsdb_demos/agent_lab/hr_documents_volume', format => 'binaryFile') \n",
    "    )\n",
    "  )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9f633c-7b39-4819-bbce-a5ff0895874b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG wsdb_demos;\n",
    "USE SCHEMA agent_lab;\n",
    "SELECT * FROM knowledge_base;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686d1dd1-a82b-419f-bff4-5116aba014df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2/ Create our vector search table\n",
    "\n",
    "### 2.1/ Vector search Endpoints\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-basic-prep-2.png?raw=true\" style=\"float: right; margin-left: 10px\" width=\"400px\">\n",
    "\n",
    "Vector search endpoints are entities where your indexes will live. Think about them as entry point to handle your search request. \n",
    "\n",
    "Let's start by creating our first Vector Search endpoint. Once created, you can view it in the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c65dcf-2a70-4699-8209-7f699384bb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de5c6fd-8e4c-4a5e-807d-95bb81fcf3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-basic-prep-3.png?raw=true\" style=\"float: right; margin-left: 10px\" width=\"400px\">\n",
    "\n",
    "\n",
    "### 2.2/ Creating the Vector Search Index\n",
    "\n",
    "Once the endpoint is created, all we now have to do is to as Databricks to create the index on top of the existing table. \n",
    "\n",
    "You just need to specify the text column and our embedding foundation model (`GTE`).  Databricks will build and synchronize the index automatically for us.\n",
    "\n",
    "Note that Databricks provides 3 type of vector search:\n",
    "\n",
    "* **Managed embeddings**: Databricks creates the embeddings for you from a text field and Databricks synchronize the Delta table to your index (what we'll use)\n",
    "* **Self managed embeddings**: You compute the embeddings yourself and save them to your Delta table  and Databricks synchronize the Delta table to your index\n",
    "* **Direct access**: you manage the VS indexation yourself (no Delta table)\n",
    "\n",
    "This can be done using the API, or in a few clicks within the Unity Catalog Explorer menu:\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7cff98-86a6-4c7b-b66a-2ef5ae119f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{dbName}.knowledge_base\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{dbName}.knowledge_base_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"id\",\n",
    "    embedding_source_column='content', #The column containing our text\n",
    "    embedding_model_endpoint_name='databricks-gte-large-en' #The embedding endpoint used to create the embeddings\n",
    "  )\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25f6b14-b225-4f5a-8511-380309e54dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3/ Try our VS index: searching for relevant content\n",
    "\n",
    "That's all we have to do. Databricks will automatically capture and synchronize new entries in your table with the index.\n",
    "\n",
    "Note that depending on your dataset size and model size, index creation can take a few seconds to start and index your embeddings.\n",
    "\n",
    "Let's give it a try and search for similar content.\n",
    "\n",
    "*Note: `similarity_search` also support a filters parameter. This is useful to add a security layer to your RAG system: you can filter out some sensitive content based on who is doing the call (for example filter on a specific department based on the user preference).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01cb949-9585-4266-901d-dd5600db0109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"What does the Northwind Health Plus benefit plan cover?\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"id\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47282c95-e151-4d1d-b2be-4f653e28e90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3/ Config our Agent\n",
    "\n",
    "Now that our index is ready, let's configure our agent to use it as a retriever tool!\n",
    "\n",
    "We'll reuse the `agent.py` and we reuse `agent_config.yaml` file: simply add the retriever configuration and our agent will add it as one of the tools available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1df506-fa12-4680-84ce-ac7997618706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml, sys, os\n",
    "import mlflow.models\n",
    "\n",
    "agent_eval_path = os.path.abspath(os.path.join(os.getcwd(), \"./ai-agent\"))\n",
    "sys.path.append(agent_eval_path)\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/agent_evaluation\")\n",
    "conf_path = os.path.join(agent_eval_path, 'agent_config.yaml')\n",
    "\n",
    "rag_chain_config = {\n",
    "    \"config_version_name\": \"model_with_retriever\",\n",
    "    \"input_example\": [\n",
    "        {\n",
    "            \"content\": \"What does the Northwind Health Plus benefit plan cover?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"llm_endpoint_name\": \"databricks-claude-3-7-sonnet\",\n",
    "    \"max_history_messages\": 20,\n",
    "    \"retriever_config\": {\n",
    "        \"description\": \"Retrieves Contoso resource human documentation, including internal documentation about HR policies, employee resources, benefits, and related topics. Use this tool for any questions about Contoso human resources documentation or HR issues.\",\n",
    "        \"index_name\": f\"{catalog}.{dbName}.knowledge_base_vs_index\",\n",
    "        \"num_results\": 1,\n",
    "        \"tool_name\": \"contoso_resource_human_docs_retriever\"\n",
    "    },\n",
    "    \"system_prompt\": \"You are a Resource Human assistant at Contoso. Answer user questions.\",\n",
    "    \"uc_tool_names\": [f\"{catalog}.{dbName}.*\"] \n",
    "}\n",
    "\n",
    "# Load existing config if present, then merge/overwrite with new values\n",
    "if os.path.exists(conf_path):\n",
    "    with open(conf_path, \"r\") as f:\n",
    "        existing_config = yaml.safe_load(f) or {}\n",
    "    existing_config.update(rag_chain_config)\n",
    "    config_to_write = existing_config\n",
    "else:\n",
    "    config_to_write = rag_chain_config\n",
    "\n",
    "# Write (create or modify) the YAML configuration file\n",
    "with open(conf_path, \"w\") as f:\n",
    "    yaml.dump(config_to_write, f, sort_keys=False)\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b4966-ba94-4d7a-aba6-363109f15401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "#Let's try our retriever to make sure we know have access to the rh documents\n",
    "request_example = \"What does the Northwind Health Plus benefit plan cover?\"\n",
    "answer = AGENT.predict({\"input\":[{\"role\": \"user\", \"content\": request_example}]})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a528d9d-b574-4f07-91bf-0a3161bf100c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now log the new agent in the MLflow model registry using `mlflow.pyfunc.log_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78fa5f8d-aa06-49a5-af7c-dbdc9e49a34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agent captures required resources for agent execution, note that it now has the VS index referenced\n",
    "for r in AGENT.get_resources():\n",
    "  print(f\"Resource: {type(r).__name__}:{r.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e380fcc-f7fa-467b-be9d-1a736998049d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=model_config.get('config_version_name')):\n",
    "  logged_agent_info = mlflow.pyfunc.log_model(\n",
    "    name=\"agent\",\n",
    "    python_model=agent_eval_path+\"/agent.py\",\n",
    "    model_config=conf_path,\n",
    "    input_example={\"input\": [{\"role\": \"user\", \"content\": request_example}]},\n",
    "     # Determine resources (endpoints, fonctions, vs...) to specify for automatic auth passthrough for deployment\n",
    "    resources=AGENT.get_resources(),\n",
    "    extra_pip_requirements=[\"databricks-connect\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431d0e2f-e55f-4bc7-975d-7b9bd6e81c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4/ Evaluate our agent against our documents base\n",
    "\n",
    "Our new model is available! As usual, the next step is to evaluate our dataset to make sure we're improving our answers.\n",
    "\n",
    "\n",
    "### 4.1/ Generate synthetic eval data\n",
    "\n",
    "Note that our eval dataset doesn't have any entry on our PDF.\n",
    "\n",
    "Using Databricks, it's easy to bootstrap our evaluation dataset with synthetic eval data, and then improve this dataset over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d461f3-44a5-4909-8713-cb5291eebe6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "docs = spark.table('knowledge_base')\n",
    "# Describe what our agent is doing\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that answers questions about Contoso's human resources documentation, including internal documentation about HR policies, employee resources, benefits, and related topics. The Agent has access to a corpus of HR Documents, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response.\n",
    "\"\"\"\n",
    "\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A Contoso employee asking about HR policies, benefits, or internal resources\n",
    "- An HR representative seeking information on procedures or role responsibilities\n",
    "\n",
    "# Example questions and answers\n",
    "- Q: What does the Northwind Health Plus benefits plan cover?\n",
    "  A: The Northwind Health Plus plan covers medical, dental, and vision care, prescription drugs, and mental health services.\n",
    "- Q: What is the procedure for taking maternity leave?\n",
    "  A: You must notify Human Resources at least 30 days before the expected date and provide the corresponding medical documentation.\n",
    "- Q: What does the PerksPlus program include?\n",
    "  A: PerksPlus includes discounts on gyms, wellness memberships, food vouchers, and financial support.\n",
    "- Q: What are the responsibilities of a data analyst according to the role library?\n",
    "  A: According to the Role Library, a data analyst is responsible for collecting, cleaning, and analyzing data to generate strategic business reports.\n",
    "- Q: What are Northwind's standard benefits?\n",
    "  A: Standard benefits include basic medical insurance, paid vacation, and a 401(k) retirement plan.\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "# Generate synthetic eval dataset\n",
    "evals = generate_evals_df(\n",
    "    docs,\n",
    "    num_evals=10,\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines\n",
    ")\n",
    "evals[\"inputs\"] = evals[\"inputs\"].apply(lambda x: {\"question\": x[\"messages\"][0][\"content\"]})\n",
    "display(evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c110f417-7aa7-413c-b14c-5e8385797dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "\n",
    "eval_dataset_table_name = f\"{catalog}.{dbName}.ai_agent_mlflow_eval\"\n",
    "\n",
    "try:\n",
    "  eval_dataset = mlflow.genai.datasets.get_dataset(eval_dataset_table_name)\n",
    "except Exception as e:\n",
    "  if 'does not exist' in str(e):\n",
    "    eval_dataset = mlflow.genai.datasets.create_dataset(eval_dataset_table_name)\n",
    "    # Add your examples to the evaluation dataset\n",
    "    eval_dataset.merge_records(evals)\n",
    "    print(\"Added records to the evaluation dataset.\")\n",
    "\n",
    "# Preview the dataset\n",
    "display(eval_dataset.to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14af03af-5bfa-44d3-8392-8083537f2091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM {catalog}.{dbName}.ai_agent_mlflow_eval\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdc0fcd-0a6a-42d8-a9a8-d43a07b3447b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2/ Running our evaluation\n",
    "As previously, let's run our evaluations using the MLFlow dataset. We'll make sure our model still behave properly on the customer-related question, and now perform well on our knowledge-base questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3368ce-ca07-4738-b7eb-e7beca5cf681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RetrievalGroundedness, RelevanceToQuery, Safety, Guidelines\n",
    "import pandas as pd\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.get_dataset(f\"{catalog}.{dbName}.ai_agent_mlflow_eval\")\n",
    "\n",
    "#Get the same scorers as previously (function is defined in _resources/01-setup, similar to the previous step)\n",
    "scorers = get_scorers()\n",
    "\n",
    "# Load the model and create a prediction function\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{logged_agent_info.run_id}/agent\")\n",
    "def predict_wrapper(question):\n",
    "    # Format for chat-style models\n",
    "    model_input = pd.DataFrame({\n",
    "        \"input\": [[{\"role\": \"user\", \"content\": question}]]\n",
    "    })\n",
    "    response = loaded_model.predict(model_input)\n",
    "    return response['output'][-1]['content'][-1]['text']\n",
    "    \n",
    "print(\"Running evaluation...\")\n",
    "with mlflow.start_run(run_name='eval_with_retriever'):\n",
    "    results = mlflow.genai.evaluate(data=eval_dataset, predict_fn=predict_wrapper, scorers=scorers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f2210a-7502-4231-ba59-9e32f2ac915e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3/ Deploy the final model! \n",
    "\n",
    "We're good to go. Let's deploy our model to UC and update our endpoint with the latest version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8727680-c29d-4554-9815-532fae6150d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "MODEL_NAME = \"ai_hr_agent_demo\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{dbName}.{MODEL_NAME}\"\n",
    "\n",
    "# register the model to UC\n",
    "client = MlflowClient()\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME, tags={\"model\": \"rh_support_agent\", \"model_version\": \"with_retriever\"})\n",
    "\n",
    "client.set_registered_model_alias(name=UC_MODEL_NAME, alias=\"model-to-deploy\", version=uc_registered_model_info.version)\n",
    "displayHTML(f'<a href=\"/explore/data/models/{catalog}/{dbName}/{MODEL_NAME}\" target=\"_blank\">Open Unity Catalog to see Registered Agent</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f9780a-c9e7-4691-bf83-5428331f96cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "endpoint_name = f'{MODEL_NAME}_{catalog}_{db}'[:60]\n",
    "\n",
    "if len(agents.get_deployments(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)) == 0:\n",
    "  agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, endpoint_name=endpoint_name, tags = {\"project\": \"rh_support_agent\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa6d106-5b3b-4663-b6a2-f17e4e0c60e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5/ Deploying our frontend App with Lakehouse Applications\n",
    "\n",
    "Now that our agent is ready, let's deploy a GradIO application to serve its content to our end users. \n",
    "\n",
    "Mosaic AI Agent Evaluation review app is used for collecting stakeholder feedback during your development process.\n",
    "\n",
    "You still need to deploy your own front end application!\n",
    "\n",
    "Let's leverage Databricks Lakehouse Applications to build and deploy our first, simple chatbot frontend app. \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-frontend-app.png?raw=true\" width=\"1200px\">\n",
    "\n",
    "\n",
    "<div style=\"background-color: #d4e7ff; padding: 10px; border-radius: 15px;\">\n",
    "<strong>Note:</strong> In this example, we'll deploy the app using the endpoint. However, if the only use-case is the app itself, you can also directly package your MLFlow Chat Agent within your application, and remove the endpoint entirely!\n",
    "</div>\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=4028599767852497&notebook=%2F04-deploy-app%2F04-Deploy-Frontend-Lakehouse-App&demo_name=ai-agent&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fai-agent%2F04-deploy-app%2F04-Deploy-Frontend-Lakehouse-App&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd8b1da-334c-462e-8b2a-6b65b1785fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add your application configuration\n",
    "\n",
    "Lakehouse apps allow you to work with any Python framework. For our demo, we'll create a simple configuration file containing the model serving endpoint name and save it as `chatbot_app/app.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70829939-c2b0-4634-9c69-17289ed05550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The Databricks APP will be using the following model serving endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ffa758-787f-43f6-9ddc-93854fa67d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Our frontend application will hit the model endpoint we deployed.\n",
    "# Because dbdemos let you change your catalog and database, let's make sure we deploy the app with the proper endpoint name\n",
    "yaml_app_config = {\"command\": [\"uvicorn\", \"main:app\", \"--workers\", \"1\"],\n",
    "                    \"env\": [{\"name\": \"MODEL_SERVING_ENDPOINT\", \"value\": ENDPOINT_NAME}]\n",
    "                  }\n",
    "try:\n",
    "    with open('chatbot_app/app.yaml', 'w') as f:\n",
    "        yaml.dump(yaml_app_config, f)\n",
    "except Exception as e:\n",
    "    print(f'pass to work on build job - {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3317df9-fb2f-4668-bfa3-3bd1c50aeac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Capturing feedback through MLFlow Tracing and Feedback API\n",
    "\n",
    "With MLFLow 3, it's now easy to directly capture feedback (thumb up/down) from your application!\n",
    "\n",
    "```\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "input_message = [{\"content\": \"test\", \"role\": \"user\", \"type\": \"message\"}]\n",
    "\n",
    "response = client.predict(\n",
    "  endpoint=ENDPOINT_NAME,\n",
    "  inputs={'input': input_message, \"databricks_options\": {\n",
    "      # Return the trace so we can get the trace_id for logging feedback. (return only the id for faster results)\n",
    "      \"return_trace\": True\n",
    "    }}\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Then, simply use the `mlflow-tracing` in your chatbot backend to send emit the trace with the user feedback:\n",
    "\n",
    "\n",
    "```\n",
    "mlflow.log_feedback(\n",
    "                trace_id=trace_id, #the trace id present in the response, typically tr-xxxxx\n",
    "                name='user_feedback',\n",
    "                value=True if like_data.liked else False,\n",
    "                rationale=None,\n",
    "                source=mlflow.entities.AssessmentSource(source_type='HUMAN', source_id='user')\n",
    "            )\n",
    "```\n",
    "\n",
    "*Note: you can also manage your own IDs - see the [feedback documentation](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/collect-user-feedback/) for more details*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3536f4d3-2906-48aa-88e4-c9c1081477dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's now create our chatbot application using Gradio using Databricks Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffcf8977-6860-406b-8f18-2a3007116e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploying our application\n",
    "\n",
    "Our application is made of 2 files under the `chatbot_app` folder:\n",
    "- `main.py` containing our python code\n",
    "- `app.yaml` containing our configuration\n",
    "\n",
    "All we now have to do is call the API to create a new app and deploy using the `chatbot_app` path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431af86a-6688-4c3c-8bec-399353694a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.apps import App, AppResource, AppResourceServingEndpoint, AppResourceServingEndpointServingEndpointPermission, AppDeployment\n",
    "\n",
    "w = WorkspaceClient()\n",
    "app_name = \"rh-ai-agent-app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f3c2f07-39f0-497e-a9e5-0ef32479e1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lakehouse apps come with an auto-provisioned Service Principal. Let's grant this Service Principal access to our model endpoint before deploying..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb39bba-9b99-49f2-a108-acf2da127f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "serving_endpoint = AppResourceServingEndpoint(name=ENDPOINT_NAME,\n",
    "                                              permission=AppResourceServingEndpointServingEndpointPermission.CAN_QUERY\n",
    "                                              )\n",
    "\n",
    "rag_endpoint = AppResource(name=\"rag-endpoint\", serving_endpoint=serving_endpoint) \n",
    "\n",
    "rag_app = App(name=app_name, \n",
    "              description=\"Your Databricks assistant\", \n",
    "              default_source_code_path=os.path.join(os.getcwd(), 'chatbot_app'),\n",
    "              resources=[rag_endpoint])\n",
    "try:\n",
    "  app_details = w.apps.create_and_wait(app=rag_app)\n",
    "  print(app_details)\n",
    "except Exception as e:\n",
    "  if \"already exists\" in str(e):\n",
    "    print(\"App already exists, you can deploy it\")\n",
    "  else:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eda563b-71c6-4841-a1f1-769c14633c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Once the app is created, we can (re)deploy the code as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c45b6e6f-acbd-4658-9f1e-9a2bf77c47f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"Shared/pdf-rag-tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a30ced-2899-41cd-b0a5-7854dfc95c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deployment = AppDeployment(source_code_path=os.path.join(os.getcwd(), 'chatbot_app'))\n",
    "\n",
    "app_details = w.apps.deploy_and_wait(app_name=app_name, app_deployment=deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "733d14bb-f84c-4750-8019-a3991a6750c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Let's access the application\n",
    "w.apps.get(name=app_name).url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d289061d-d5a1-4fec-8a89-8e977aac364c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Your Lakehouse app is ready and deployed!\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-gradio-app.png?raw=true\" width=\"750px\" style=\"float: right; margin-left:10px\">\n",
    "\n",
    "Open the UI to start requesting your chatbot.\n",
    "\n",
    "As improvement, we could improve our chatbot UI to provide feedback and send it to Mosaic AI Quality Labs, so that bad answers can be reviewed and improved.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We saw how Databricks provides an end to end platform: \n",
    "- Building and deploying an endpoint\n",
    "- Buit-in solution to review, analyze and improve our chatbot\n",
    "- Deploy front-end genAI application with lakehouse apps!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4776506731743861,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab2_ProCode_Agent_with_Databricks.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
